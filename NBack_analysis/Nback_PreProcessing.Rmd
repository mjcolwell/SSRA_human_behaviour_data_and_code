---
title: "N-back Preprocessing"
date:"24/02/2022"
output: html_document
---
## N-back RMarkdown (February 24, 2022; Version 0.1)

This is an RMarkdown to allow for reproducible and accurate preprocessing of the N-back Psychopy 
task created by Michael Colwell (michael.colwell@psych.ox.ac.uk / ORCID 0000-0001-7846-2879).

The task materials and preprocessing script are offered free of charge for researchers. **It is requested that researchers who publish data using these materials (task or preprocessing script) cite the code for the task
in relevant publications**. Our reference is:

Colwell, Michael, Tagomori, Hosana, Martens, Marieke, Murphy, Susannah, & Harmer, Catherine. (2023). N-Back (Oxford) - fMRI and non-scanner release (Python). Zenodo. https://doi.org/10.5281/zenodo.8003407

##Required R packages

You will need the following packages installed and loaded before executing the below code chunks. 

```{r libraries, echo=FALSE, include=FALSE}
library(dplyr)
library(tidyverse)
library(gtools)
library(knitr)
library(stringr)
library(purrr)
library(readxl)
library(data.table)
library(openxlsx)
library(ggpubr)
library(rstatix)
library("ez")
library(ggsignif)
library(RColorBrewer)
library(emmeans)
library(plotrix)
library(lme4)
library(stringr)
library(effectsize)
library(lmerTest)
library(cowplot)
```

##Begin preprocessing: Setting directory, merging files and deleting unnecessary columns

You will first need to point to the directory of your task files, typically in the 'data' subfolder where the
psychopy task is located. **Please edit the path directory below after the setwd function.**

The next lines in the chunk of code will allow you to merge all data files (.csv) in the directory assigned above to a dataframe, and then delete extraneous columns from the dataframe.

**Potential error**: "Error in rbind(deparse.level, ...) : numbers of columns of arguments do not match"
**Solution**: One of the .csv files may not have run beyond the practice section, therefore not enough columns
have generated. Delete this file and then continue.

```{r b0, echo=FALSE, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls()) # clear the workspace

setwd("C:/Users/micha/Desktop/NBack_analysis")

N_back_files <- list.files(pattern = glob2rx("*Back*.csv")) # create a list of files containing the word 'Back'.

N_back <- N_back_files %>% map_dfr(read.csv) # stack the files containing the word 'Back' on top of each other.

N_back <- N_back %>% rename(Task.version = Task.Version..required.to.run., Participant.ID = participant)

N_back <- N_back %>% dplyr::select(Participant.ID, PRE.POST, Task.version, Instructions, ISIjitterPractice, CorrectnessP, KeyPractice.corr, KeyPractice.rt, Correctness, key_resp.corr, key_resp.rt, ISIjitter, CondFile, CondFileP, LettersDisplayed)

# Note that the 'correctness'; column refers to the actual key they should have pressed for the actual trials and that 'correctnessP' refers to the actual key they should have pressed for the practice trials.

N_back <- N_back %>%
  mutate(key_resp.rt = key_resp.rt * 1000, ISI.time = ISIjitter * 100, ISI.time.pract = ISIjitterPractice * 100) # change the reaction times to ms.

N_back <- N_back %>% mutate(ISI.combined = coalesce(ISI.time, ISI.time.pract)) # combining the ISI time and ISI practice together into one column.

N_back <- N_back %>% mutate(Condition.Files.Combined = coalesce(CondFileP, CondFile))
# combining the CondFileP and CondFile columns together into one column.

N_back <- N_back %>% mutate(Correct.Combined = coalesce(KeyPractice.corr, key_resp.corr)) # combining the keyPractice.corr and key_resp.corr columns into a single column.

N_back <- N_back %>% mutate(Response.Time.Combined = coalesce(KeyPractice.rt, key_resp.rt)) # combining the keyPractice.rt and key_resp.rt columns into a single column.
```
##Further parsing the dataframe 

The following chunks will allow you to further parse the data, including generating omission/comission error checks
based on conditions set in the file.

```{r b0, echo=FALSE, include=TRUE}
N_back_first_tidy <- N_back %>%
  mutate(
    Result = case_when(
      (Correctness == "None" & Correct.Combined == "1") ~ "snp_TRUE",
      (Correctness == "None" & Correct.Combined == "0") ~ "snp_FALSE",
      (Correctness == "space" & Correct.Combined == "1") ~ "sp_TRUE",
      (Correctness == "space" & Correct.Combined == "0") ~ "sp_FALSE"
    ),
    n_back = case_when(
      (Instructions == "Rule: Press 'space' when you see the letter 'x' or 'X'.") ~ "0-back",
      (Instructions == "New rule: Press 'space' when you see the letter 'x' or 'X'.") ~ "0-back",
      (Instructions == "New rule: Press 'space' when you see 1-back (i.e. the same letter as the one that appeared 1 letter ago.)") ~ "1-back",
      (Instructions == "New rule: Press 'space' when you see 2-back (i.e. the same letter as the one that appeared 2 letters ago.)") ~ "2-back",
      (Instructions == "New rule: Press 'space' when you see 3-back (i.e. the same letter as the one that appeared 3 letters ago.)") ~ "3-back"
    )
  )

# Outputing the result & identifying the number of "n" in n-back tests and output as the form of "n-back"

N_back_first_tidy <- N_back_first_tidy %>% dplyr::select(Participant.ID, PRE.POST, n_back, LettersDisplayed, Correct.Combined, Result, Response.Time.Combined)
# Selecting the necessary items

N_back_first_tidy <- N_back_first_tidy %>% rename(CharacterDisplayed = LettersDisplayed, KeyResponse_correct = Correct.Combined, ResponseTime = Response.Time.Combined)
# Renaming the columns

# write.csv(N_back_summary, outputname, row.names = FALSE) #write to a new csv file

## Calculating the correctness

N_back_Correctness_Total <- N_back_first_tidy %>%
  mutate(
    snp_TRUE_total = case_when(
      (Result == "snp_TRUE") ~ 1,
      (Result == "snp_FALSE") ~ 0
    ),
    snp_FALSE_total = case_when(
      (Result == "snp_TRUE") ~ 0,
      (Result == "snp_FALSE") ~ 1
    ),
    sp_TRUE_total = case_when(
      (Result == "sp_TRUE") ~ 1,
      (Result == "sp_FALSE") ~ 0
    ),
    sp_FALSE_total = case_when(
      (Result == "sp_TRUE") ~ 0,
      (Result == "sp_FALSE") ~ 1
    )
  )
# Transforming the output to the "0/1" form

## Extract the response time only when SP_TRUE = 1 (hit response time)
## the response time originally include the hit response time and the false hit response time
N_back_Correctness_Total <- N_back_Correctness_Total %>%
  transform(RT_Hits = ifelse(sp_TRUE_total == 1, ResponseTime, NA))
# create RT_Hits, when sp_TRUE_total = 1, show response time, else, show NA (which will now be counted on the table)

N_back_Correctness_Total <- N_back_Correctness_Total %>% dplyr::select(Participant.ID, PRE.POST, n_back, snp_TRUE_total, snp_FALSE_total, sp_TRUE_total, sp_FALSE_total, ResponseTime, RT_Hits)
# Selecting the necessary items


## Summarising scores according to participant ID and n-back
N_back_Summary_Table <- N_back_Correctness_Total %>%
  group_by(Participant.ID, n_back, PRE.POST) %>%
  summarize(
    mean_RT = mean(RT_Hits, na.rm = TRUE),
    SP_TRUE_acc = sum(sp_TRUE_total, na.rm = TRUE),
    SP_FALSE_acc = sum(sp_FALSE_total, na.rm = TRUE),
    SNP_FALSE_acc = sum(snp_FALSE_total, na.rm = TRUE),
    SNP_TRUE_acc = sum(snp_TRUE_total, na.rm = TRUE)
  )
```

```{r b0, echo=FALSE, include=TRUE}
## Data quality checks

N_back_Summary_Table2 <- N_back_Correctness_Total %>%
  group_by(Participant.ID, PRE.POST) %>%
  summarize(
    mean_RT = mean(RT_Hits, na.rm = TRUE),
    SP_TRUE_acc = sum(sp_TRUE_total, na.rm = TRUE),
    SP_FALSE_acc = sum(sp_FALSE_total, na.rm = TRUE),
    SNP_FALSE_acc = sum(snp_FALSE_total, na.rm = TRUE),
    SNP_TRUE_acc = sum(snp_TRUE_total, na.rm = TRUE)
  )

N_back_Summary_Table2 <- N_back_Summary_Table2 %>% filter(!grepl("NaN", mean_RT))

is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 2.0 * IQR(x) | x > quantile(x, 0.75) + 2.0 * IQR(x))
}

N_back_Summary_Table2 %>% mutate(PRE.POST = as.character(PRE.POST))

N_back_Summary_Table2 %>%
  group_by(PRE.POST) %>%
  mutate(outlier = ifelse(is_outlier(SP_TRUE_acc), PRE.POST, "NA")) %>%
  ggplot(aes(x = factor(PRE.POST), SP_TRUE_acc)) +
  geom_boxplot(outlier.colour = NA) +
  scale_x_discrete(limits = unique(rev(N_back_Summary_Table2$PRE.POST))) +
  ggbeeswarm::geom_beeswarm(aes(color = SP_TRUE_acc)) +
  ggrepel::geom_text_repel(data = . %>% filter(!is.na(outlier)), aes(label = Participant.ID)) +
  scale_colour_gradient(low = "blue", high = "red") +
  labs(title = "Participant average accuracy for all 'should press key' trials") +
  ylab("Accuracy") +
  xlab("Pre or Post")

N_back_Summary_Table2 %>%
  group_by(PRE.POST) %>%
  mutate(outlier = ifelse(is_outlier(SNP_TRUE_acc), PRE.POST, "NA")) %>%
  ggplot(aes(x = factor(PRE.POST), SNP_TRUE_acc)) +
  geom_boxplot(outlier.colour = NA) +
  scale_x_discrete(limits = unique(rev(N_back_Summary_Table2$PRE.POST))) +
  ggbeeswarm::geom_beeswarm(aes(color = SNP_TRUE_acc)) +
  ggrepel::geom_text_repel(data = . %>% filter(!is.na(outlier)), aes(label = Participant.ID)) +
  scale_colour_gradient(low = "blue", high = "red") +
  labs(title = "Participant average accuracy for all 'should not press key' trials") +
  ylab("Accuracy") +
  xlab("Pre or Post")

N_back_Summary_Table2 %>%
  group_by(PRE.POST) %>%
  mutate(outlier = ifelse(is_outlier(mean_RT), PRE.POST, "NA")) %>%
  ggplot(aes(x = factor(PRE.POST), mean_RT)) +
  geom_boxplot(outlier.colour = NA) +
  scale_x_discrete(limits = unique(rev(N_back_Summary_Table2$PRE.POST))) +
  ggbeeswarm::geom_beeswarm(aes(color = mean_RT)) +
  ggrepel::geom_text_repel(data = . %>% filter(!is.na(outlier)), aes(label = Participant.ID)) +
  scale_colour_gradient(low = "blue", high = "red") +
  labs(title = "Participant average response time for all trials") +
  ylab("Accuracy") +
  xlab("Pre or Post")
``````

```{r b0, echo=FALSE, include=TRUE}
setwd("C:/Users/micha/Desktop/DemographicData")

Demographics <- read.xlsx("Demo4Analysis.xlsx")

Demographics$Participant.ID <- as.factor(Demographics$Participant.ID)

MasterNBackWide <- merge(N_back_Summary_Table, Demographics, by = "Participant.ID")

MasterNBackWide <- na.omit(MasterNBackWide)

MasterNBackWide$PRE.POST <- recode_factor(MasterNBackWide$PRE.POST, "Pre" = "Baseline")

MasterNBackWidePostOnly <- MasterNBackWide %>% filter(!str_detect(PRE.POST, "Baseline"))
```


```{r b0, echo=FALSE, include=TRUE}
# Removal of outliers from dataset prior to analysis. These participants were both outliers in pre-unblinding checks and self-reported difficulty in understanding task rules.

MasterNBackWide$IDvisit <- paste(MasterNBackWide$Participant.ID, MasterNBackWide$PRE.POST)

removal_df <- subset(MasterNBackWide, IDvisit != "P002 Baseline" & IDvisit != "P002 Post" & IDvisit != "P035 Baseline" & IDvisit != "P035 Post" & IDvisit != "P009 Baseline" & IDvisit != "P009 Post")

MasterNBackWide <- droplevels(removal_df)
```

```{r b0, echo=FALSE, include=TRUE}
# Create column for participant.id + visit + & n_back level + id
MasterNBackWide$IDvisit <- paste(MasterNBackWide$Participant.ID, MasterNBackWide$PRE.POST)

MasterNBackWide$IDnback <- paste(MasterNBackWide$Participant.ID, MasterNBackWide$n_back)

# Split datasets into PRE and POST

MasterNBackWidePostOnly <- MasterNBackWide %>% filter(!str_detect(PRE.POST, "Baseline"))
MasterNBackWidePreOnly <- MasterNBackWide %>% filter(!str_detect(PRE.POST, "Post"))
```

#Graphs for report


```{r}
Figure1 <- MasterNBackWidePostOnly %>%
  group_by(PRE.POST, Allocation, n_back) %>%
  summarize(value = mean(mean_RT), SE = std.error(mean_RT)) %>%
  ggplot(aes(n_back, value, group = Allocation)) +
  scale_color_brewer(palette = "Set2") +
  geom_errorbar(aes(x = n_back, ymin = value - SE, ymax = value + SE, color = Allocation), width = 0.25) +
  geom_point(aes(color = Allocation, shape = Allocation), size = 2.4, alpha = 0.8) +
  geom_line(aes(color = Allocation, linetype = Allocation), size = 1.4) +
  geom_ribbon(aes(
    ymin = value - SE,
    ymax = value + SE, fill = Allocation
  ), color = NA, alpha = 0.095, show.legend = FALSE) +
  geom_signif(
    comparisons = list(c("ACTIVE", "PLACEBO")),
    p.adjust.method = "bonferroni",
    map_signif_level = c("***" = 0.001, "**" = 0.01, "*" = 0.05, " " = 2),
    margin_top = 0.05
  ) +
  scale_color_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "") +
  ylab("Response time (ms)\n") +
  xlab("\nWorking memory load level (n-back)\n\n") +
  theme_minimal() +
  theme(legend.position = "none", axis.text = element_text(size = 14), axis.title = element_text(size = 16)) +
  scale_shape_manual(values = c(19, 15)) +
  geom_segment(aes(x = 4.2, y = 812, xend = 4.2, yend = 700, group = "segment")) +
  geom_segment(aes(x = 4.15, y = 811.5, xend = 4.2, yend = 811.5, group = "segment")) +
  geom_segment(aes(x = 4.15, y = 700.5, xend = 4.2, yend = 700.5, group = "segment")) +
  geom_text(aes(x = 4.285, label = "*", y = 755), colour = "Black", size = 5)

# Accuracy

Figure2 <- MasterNBackWidePostOnly %>%
  group_by(PRE.POST, Allocation, n_back) %>%
  summarize(value = mean(SP_TRUE_acc) / 10 * 100, SE = std.error(SP_TRUE_acc) / 10 * 100) %>%
  ggplot(aes(n_back, value, group = Allocation)) +
  scale_color_brewer(palette = "Set2") +
  geom_errorbar(aes(x = n_back, ymin = value - SE, ymax = value + SE, color = Allocation), width = 0.25) +
  geom_point(aes(color = Allocation, shape = Allocation), size = 2, alpha = 0.8) +
  geom_line(aes(color = Allocation, linetype = Allocation), size = 1.4) +
  geom_ribbon(aes(
    ymin = value - SE,
    ymax = value + SE, fill = Allocation
  ), color = NA, alpha = 0.095, show.legend = FALSE) +
  ylim(50, 100) +
  scale_shape_manual(values = c(19, 15)) +
  scale_color_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  geom_signif(
    comparisons = list(c("ACTIVE", "PLACEBO")),
    p.adjust.method = "bonferroni",
    map_signif_level = c("***" = 0.001, "**" = 0.01, "*" = 0.05, " " = 2),
    margin_top = 0.05
  ) +
  labs(title = "") +
  ylab("Accuracy for target hits (%)\n") +
  xlab("Working memory load level (n-back)\n\n") +
  theme_minimal() +
  theme(legend.position = "none", axis.text = element_text(size = 14), axis.title = element_text(size = 16))


# Baseline figures

Figure3 <- MasterNBackWidePreOnly %>%
  group_by(PRE.POST, Allocation, n_back) %>%
  summarize(value = mean(mean_RT), SE = std.error(mean_RT)) %>%
  ggplot(aes(n_back, value, group = Allocation)) +
  scale_color_brewer(palette = "Set2") +
  geom_errorbar(aes(x = n_back, ymin = value - SE, ymax = value + SE, color = Allocation), width = 0.25) +
  geom_point(aes(color = Allocation, shape = Allocation), size = 2.5, alpha = 0.8) +
  geom_line(aes(color = Allocation, linetype = Allocation), size = 1.2) +
  geom_ribbon(aes(
    ymin = value - SE,
    ymax = value + SE, fill = Allocation
  ), color = NA, alpha = 0.095, show.legend = FALSE) +
  geom_signif(
    comparisons = list(c("ACTIVE", "PLACEBO")),
    p.adjust.method = "bonferroni",
    map_signif_level = c("***" = 0.001, "**" = 0.01, "*" = 0.05, " " = 2),
    margin_top = 0.05
  ) +
  scale_color_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "") +
  ylab("Response time (ms)\n") +
  xlab("Working memory load level (n-back) \n") +
  theme_minimal() +
  scale_shape_manual(values = c(19, 15)) +
  theme(legend.position = "bottom", axis.text.y = element_text(size = 15), axis.text.x = element_text(size = 15), axis.title.y = element_text(size = 20), axis.title.x = element_text(size = 20), legend.title = element_text(size = 18), legend.text = element_text(size = 14), legend.key.size = unit(1, "cm"))

#

Figure4 <- MasterNBackWidePreOnly %>%
  group_by(PRE.POST, Allocation, n_back) %>%
  summarize(value = mean(SP_TRUE_acc) / 10 * 100, SE = std.error(SP_TRUE_acc) / 10 * 100) %>%
  ggplot(aes(n_back, value, group = Allocation)) +
  scale_color_brewer(palette = "Set2") +
  geom_errorbar(aes(x = n_back, ymin = value - SE, ymax = value + SE, color = Allocation), width = 0.25) +
  geom_point(aes(color = Allocation, shape = Allocation), size = 2.5, alpha = 0.8) +
  geom_line(aes(color = Allocation, linetype = Allocation), size = 1.2) +
  geom_ribbon(aes(
    ymin = value - SE,
    ymax = value + SE, fill = Allocation
  ), color = NA, alpha = 0.095, show.legend = FALSE) +
  ylim(50, 100) +
  scale_shape_manual(values = c(19, 15)) +
  scale_color_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  geom_signif(
    comparisons = list(c("ACTIVE", "PLACEBO")),
    p.adjust.method = "bonferroni",
    map_signif_level = c("***" = 0.001, "**" = 0.01, "*" = 0.05, " " = 2),
    margin_top = 0.05
  ) +
  labs(title = "") +
  ylab("Accuracy for target hits (%)\n") +
  xlab("Working memory load level (n-back)\n\n") +
  theme_minimal() +
  theme(legend.position = "none", axis.text.y = element_text(size = 15), axis.text.x = element_text(size = 15), axis.title.y = element_text(size = 20), axis.title.x = element_text(size = 20))

CombinedPlots <- plot_grid(Figure3, Figure4, align = "h", ncol = 2, labels = c("A", "B"))
```

##ANCOVA pre-processing and analysis

```{r pressure, echo=FALSE}
ANCOdf1 <- MasterNBackWidePostOnly[c("Allocation", "Participant.ID", "SP_TRUE_acc", "SNP_TRUE_acc", "mean_RT", "n_back", "Gender")]

ANCOdf1 <- rename(ANCOdf1, SP_TRUE_acc_Post = SP_TRUE_acc)
ANCOdf1 <- rename(ANCOdf1, SNP_TRUE_acc_Post = SNP_TRUE_acc)
ANCOdf1 <- rename(ANCOdf1, RTPost = mean_RT)

ANCOdf2 <- MasterNBackWidePreOnly[c("Allocation", "Participant.ID", "SP_TRUE_acc", "SNP_TRUE_acc", "mean_RT", "n_back", "Gender")]

ANCOdf2 <- rename(ANCOdf2, SP_TRUE_acc_Pre = SP_TRUE_acc)
ANCOdf2 <- rename(ANCOdf2, SNP_TRUE_acc_Pre = SNP_TRUE_acc)
ANCOdf2 <- rename(ANCOdf2, RTPre = mean_RT)

ANCOdfcomp <- left_join(ANCOdf1, ANCOdf2, by = c("Participant.ID", "Allocation", "n_back", "Gender"))

# Accuracy analysis

ancova_model <- aov(SP_TRUE_acc_Post ~ Allocation + n_back + SP_TRUE_acc_Pre + Allocation * n_back + Error(Participant.ID), data = ANCOdfcomp)

summary(ancova_model)

##

lm_model <- lmer(SP_TRUE_acc_Post ~ Allocation + n_back + Allocation:n_back + RTPre + (1 | Participant.ID), data = ANCOdfcomp)

eta_squared(lm_model, ci = 0.95, alternative = "two.sided")

# Gender analysis

ancova_model <- aov(SP_TRUE_acc_Post ~ Allocation + Gender + Allocation:Gender + n_back + n_back:Allocation + n_back:Allocation:Gender + SP_TRUE_acc_Pre + Error(Participant.ID), data = ANCOdfcomp)

summary(ancova_model)

ANCOdf1 %>%
  group_by(Gender, Allocation) %>%
  get_summary_stats(SP_TRUE_acc_Post, type = "mean_sd")

## RT analysis

ancova_model2 <- aov(RTPost ~ Allocation + n_back + RTPre + Allocation * n_back + Error(Participant.ID), data = ANCOdfcomp)

summary(ancova_model2)

lm_model <- lmer(RTPost ~ Allocation + n_back + Allocation:n_back + RTPre + (1 | Participant.ID), data = ANCOdfcomp)

eta_squared(lm_model, ci = 0.95, alternative = "two.sided")

# Significant effect - proceed to analysing post data as post hoc

# Calculate estimated marginal means (EMMs) for Allocation*Condition interaction
lm_model <- lm(RTPost ~ Allocation + n_back + Allocation:n_back, data = ANCOdfcomp)
EMM_2 <- emmeans(lm_model, ~ Allocation | n_back)

# Calculate pairwise comparisons for the specified contrasts
pairwise_comparisons <- pairs(EMM_2, adjust = "holm")

# Print the results
summary(pairwise_comparisons)

# Calculate effect size using eff_size
effect_size <- eff_size(EMM_2, sigma = sigma(lm_model), edf = df.residual(lm_model))

# Print the effect size summary
summary(effect_size)

ANCOdfcomp %>%
  group_by(n_back, Allocation) %>%
  get_summary_stats(RTPost, type = "mean_sd")

# Gender analysis

ANCOVA_Post_RT <- aov(RTPost ~ Allocation + Gender + Allocation:Gender + n_back + n_back:Allocation + n_back:Allocation:Gender + RTPre + Error(Participant.ID), data = ANCOdfcomp)

summary(ANCOVA_Post_RT)

ANCOdf1 %>%
  group_by(Gender, Allocation) %>%
  get_summary_stats(RTPost, type = "mean_sd")

#####
```
